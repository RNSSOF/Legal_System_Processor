import os
import re
import yaml
import json
import traceback
import time
from pathlib import Path
from google import genai
from google.genai.errors import APIError

# --- ุซูุงุจุช ูุฅุนุฏุงุฏุงุช ---
MAX_RETRIES = 3 # ุนุฏุฏ ุงููุญุงููุงุช ุงููุตูู ููุงุชุตุงู ุจู Gemini

# --- ุชูุงุจุน ูุณุงุนุฏุฉ ---

def create_yaml_header(data):
    """ุฅูุดุงุก ุฑุฃุณ YAML ุจุชูุณูู ุตุญูุญ"""
    return "---\n" + yaml.dump(data, allow_unicode=True, sort_keys=False) + "---\n"

def save_ocr_review_file(doc_slug, all_corrections, output_path):
    """ูุธููุฉ ูุฅูุดุงุก ููู ocr_review.json - ุชุณุชุฎุฏู ูุณุงุฑ ุงููุฌูุฏ ุงููุฑุนู (output_path)"""
    
    if not all_corrections:
        return 
        
    total_errors = len(all_corrections)
    affected_files = len(set(c['file'] for c in all_corrections))
    
    review_data = {
        "doc": doc_slug,
        "review_summary": {
            "total_potential_errors": total_errors,
            "files_affected": affected_files
        },
        "corrections_to_review": all_corrections
    }
    
    output_file_path = Path(output_path) / f"{doc_slug}.ocr_review.json"
    
    with open(output_file_path, 'w', encoding='utf-8') as f:
        json.dump(review_data, f, ensure_ascii=False, indent=2)
    
    print(f" ยโ ุชู ุฅูุดุงุก ููู ุงููุฑุงุฌุนุฉ: {output_file_path.name}")


# *******************************************************************
# ุฏุงูุฉ ุงุณุชุฎูุงุต ุงูุณูุงู ุงูุฃุณุงุณู
# *******************************************************************

def get_core_context(doc_slug, source_folder="source_files"):
    """
    ุงุณุชุฎูุงุต ุงูุนููุงูุ ูุงููุต ุงูุงูุชุชุงุญูุ ูุงูุชุนุฑููุงุช (ุงููุงุฏุฉ ุงูุฃููู) ูู ุงูููู ุงูุฃุตูู.
    """
    original_file_name = f"{doc_slug.replace('ูุซููุฉ-', '')}.md"
    original_file_path = Path(source_folder) / original_file_name
    
    if not original_file_path.exists():
        print(f" ยโ ูู ูุชู ุงูุนุซูุฑ ุนูู ุงูููู ุงูุฃุตูู: {original_file_path.name}. ุณูุชู ุงุณุชุฎุฏุงู ุฅุซุฑุงุก ูุนุฒูู.")
        return None
        
    with open(original_file_path, 'r', encoding='utf-8') as f:
        full_content = f.read()

    context_match = re.search(r'(.*?)(?=^## ุงููุต ุงููุงูู ููููุงุฏ)', full_content, re.DOTALL | re.MULTILINE)
    
    if context_match:
        context_part = context_match.group(1).strip()
    else:
        context_part = full_content[:1000]

    definitions_match = re.search(r'(## ุงููุงุฏุฉ 1.*?)(?=## ุงููุงุฏุฉ 2)', full_content, re.DOTALL | re.MULTILINE)
    
    if definitions_match:
        definitions_part = definitions_match.group(1).strip()
    else:
        definitions_part = " [ูู ูุชู ุงูุนุซูุฑ ุนูู ูุงุฏุฉ ุชุนุฑููุงุช ูุงุถุญุฉ ูู ุงููุงุฏุฉ 1]"
        
    
    core_context = (
        f"--- ุงูุณูุงู ุงููุงูููู ุงูุฃุณุงุณู (ูุชุญููู ุฏููู) ---\n"
        f"{context_part}\n"
        f"--- ููุงูุฉ ุงูุณูุงู ุงูุฃุณุงุณู ---\n"
        f"--- ูุญุชูู ูุงุฏุฉ ุงูุชุนุฑููุงุช ุงููุญุชููุฉ ---\n"
        f"{definitions_part}\n"
        f"--- ููุงูุฉ ูุญุชูู ุงูุชุนุฑููุงุช ---\n"
    )
    
    print(f" ย> โ ุชู ุงุณุชุฎูุงุต ุงูุณูุงู ุงูุฃุณุงุณู ุจูุฌุงุญ ูู {original_file_path.name}")
    return core_context


# *******************************************************************
# ### [ุชุนุฏูู ุฑุฆูุณู] ุฏุงูุฉ ุงูุงุชุตุงู ุจู Gemini ูุน ุญุณุงุจ ุงูุชูููุงุช
# *******************************************************************

def call_gemini_api(article_text, core_context):
    """ูุธููุฉ ุงูุงุชุตุงู ุงููุนูู ุจู Gemini API ูุงุณุชุฎูุงุต ุงูุจูุงูุงุช ุงููุตููุฉ ูุน ุขููุฉ ุฅุนุงุฏุฉ ุงููุญุงููุฉ ูุญุณุงุจ ุงูุชูููุงุช."""
    
    if not os.getenv("GEMINI_API_KEY"):
        raise ValueError("ูุฑุฌู ุชุนููู ูุชุบูุฑ ุงูุจูุฆุฉ GEMINI_API_KEY ูุจู ุงูุชุดุบูู.")
    
    client = genai.Client()
    
    # 1. ุชุญุฏูุซ System Prompt
    system_prompt = (
        "ุฃูุช ูุญูู ูุงูููู ุฎุจูุฑ ูู ูุนุงูุฌุฉ ูุตูุต ุงูููุงููู ูุงูุฃูุธูุฉ ูุฅูุดุงุก ุจูุงูุงุช ูุตููุฉ (Metadata) ุฏูููุฉ. "
        "ูููุชู ูู ูุฑุงุกุฉ ูุต ุงููุงุฏุฉ ุงููุฑูู ูุฅุฎุฑุงุฌ ุงูุจูุงูุงุช ุงููุทููุจุฉ ูู ุตูุบุฉ JSON ููุทุ ุฏูู ุฃู ููุฏูุงุช ุฃู ุดุฑุญ. "
        "ูุฌุจ ุฃู ุชููู ุนูููุฉ ุงููุณุฎ ูููููุงุช ุงูุฃุตููุฉ ุญุฑููุฉ ูุบุฑุถ ุชุตุญูุญ ุงูู OCR. "
        "ุฑูุฒ ุนูู ุงุณุชุฎูุงุต ุงููุนูููุงุช ุจุญุฏ ุฃูุตู ููุญุฌู (ููุฎุต 30 ูููุฉุ 5-8 ูููุงุช ููุชุงุญูุฉ)."
    )
    
    # 2. ุชุญุฏูุซ User Prompt ูุฏูุฌ ุงูุณูุงู ุงูุฃุณุงุณู [Contextual Enrichment]
    user_prompt = f"""
    **[ูุงู] ูุฑุฌู ุงุณุชุฎุฏุงู ุงูุณูุงู ุงููุงูููู ุงูุฃุณุงุณู ุฃุฏูุงู ูู ุชุญููู ุงููุงุฏุฉ ุงููุงููููุฉ:**
    {core_context if core_context else 'ูุง ููุฌุฏ ุณูุงู ุฃุณุงุณูุ ุชุนุงูู ูุน ุงููุงุฏุฉ ููุซููุฉ ูุณุชููุฉ.'}

    ุจูุงุกู ุนูู ูุฐุง ุงูุชุญููู ูุงูุณูุงูุ ุฃุฎุฑุฌ ุงูุจูุงูุงุช ุงููุทููุจุฉ ุจุตูุบุฉ JSON ููุต ุงููุงุฏุฉ ุงููุงููููุฉ ุงูุชุงูู:

    ุงููุต:
    ---
    {article_text}
    ---
    
    ุงูุจูุงูุงุช ุงููุทููุจุฉ ูู JSON:
    {{
      "summary": "ููุฎุต ููุซู ูููุงุฏุฉ (30 ูููุฉ ูุญุฏ ุฃูุตู) ูุน ูุฑุงุนุงุฉ ุงูุชุนุฑููุงุช ุงููุงุฑุฏุฉ ูู ุงูุณูุงู.",
      "keywords": ["ูููุฉ ููุชุงุญูุฉ 1", "ูููุฉ ููุชุงุญูุฉ 2", "ูููุฉ ููุชุงุญูุฉ 3", ...],
      "aspect": "ุชุตููู ุงููุงุฏุฉ ูู ูู 'ุฅุฌุฑุงุฆู' (ูุดุฑุญ ุฎุทูุงุช/ุฅุฌุฑุงุกุงุช) ุฃู 'ููุถูุนู' (ูุดุฑุญ ุญููู/ูุงุฌุจุงุช/ุชุนุฑููุงุช).",
      "ocr_corrections": [
        {{
          "original_word": "ุงููููุฉ ุงูุฃุตููุฉ ุงูุฎุงุทุฆุฉ",
          "suggested_correction": "ุงูุชุตุญูุญ ุงูููุชุฑุญ",
          "context": "ุงูุฌููุฉ ุงููุญูุทุฉ ูุชุฃููุฏ ุณูุงู ุงูุฎุทุฃ"
        }}
        # ุฃุถู ูู ุฃุฎุทุงุก OCR ุงููุญุชููุฉ
      ]
    }}
    """
    
    # ----------------------------------------------------
    # ### [ุฅุถุงูุฉ ุญุณุงุจ ุชูููุงุช ุงููุฏุฎู]
    # ----------------------------------------------------
    contents_to_count = [
        {"role": "system", "parts": [{"text": system_prompt}]},
        {"role": "user", "parts": [{"text": user_prompt}]}
    ]
    
    try:
        # ุญุณุงุจ ุงูุชูููุงุช ุงููุฏุฎูุฉ ูุจู ุงูุจุฏุก ุจุงูููุงููุฉ ุงููุนููุฉ
        token_count_response = client.models.count_tokens(
            model='gemini-2.5-flash',
            contents=contents_to_count
        )
        input_tokens = token_count_response.total_tokens
    except Exception as e:
        input_tokens = 0
        print(f" ยโ๏ธ ูุดู ุญุณุงุจ ุชูููุงุช ุงููุฏุฎู: {e}. ุณูุชู ุงูุชุฑุงุถ 0.")

    # ----------------------------------------------------
    
    
    for attempt in range(MAX_RETRIES):
        print(f" ย... ุฌุงุฑู ุงูุงุชุตุงู ุจู Gemini API ููุนุงูุฌุฉ ุงูุจูุงูุงุช (ุงููุญุงููุฉ {attempt + 1}/{MAX_RETRIES})...")
        try:
            response = client.models.generate_content(
                model='gemini-2.5-flash',
                # ููุงุญุธุฉ: ุชู ุชุนุฏูู contents ูุฅุฑุณุงู ุงูู user_prompt ููุท ูุฃู ุงูู system_instruction ุชู ูุถุนู ูู config
                contents=[user_prompt],
                config={"system_instruction": system_prompt, "response_mime_type": "application/json", "temperature": 0.0}
            )
            
            # ----------------------------------------------------
            # ### [ุงุณุชุฎูุงุต ุชูููุงุช ุงููุฎุฑุฌ]
            # ----------------------------------------------------
            usage_metadata = response.usage_metadata
            # ุชูููุงุช ุงููุฑุดุญูู (candidates) ูู ูุง ููุซู ุงูุฑุฏ ุงูููุงุฆู ููููุฏูู
            output_tokens = usage_metadata.candidates_token_count
            
            llm_data = json.loads(response.text.strip())
            
            # [ุชุนุฏูู ุงูุฅุฑุฌุงุน] ููุนูุฏ ุงูุจูุงูุงุช ูุงูุชูููุงุช
            return llm_data, input_tokens, output_tokens
            
        except APIError as e:
            if 'permission denied' in str(e).lower() or '403' in str(e):
                raise APIError("ุฎุทุฃ 403: ููุชุงุญ API ุบูุฑ ุตุงูุญ ุฃู ุบูุฑ ูุณููุญ ุจู. ูุฑุฌู ุงูุชุฃูุฏ ูู ุตูุงุญูุฉ ุงูููุชุงุญ.")
            # ุงุณุชุซูุงุก ุบูุฑ ูุงุฏุญ ูุณูุญ ุจุฅุนุงุฏุฉ ุงููุญุงููุฉ
            if attempt < MAX_RETRIES - 1:
                print(f" ยโ๏ธ ูุดู ุงูุงุชุตุงูุ ุณูุนุงุฏ ุงููุญุงููุฉ ุจุนุฏ 5 ุซูุงูู: {e}")
                time.sleep(5) # ุงูุงูุชุธุงุฑ ูุจู ุงููุญุงููุฉ ุงูุชุงููุฉ
            else:
                raise APIError(f"โ ูุดู ุงูุงุชุตุงู ุจู Gemini API ุจุนุฏ {MAX_RETRIES} ูุญุงููุงุช: {e}")
                
        except json.JSONDecodeError:
            print(f" ยโ๏ธ ุชุญุฐูุฑ: ูุดู ุชุญููู JSON ูู ุฑุฏ ุงูููุฏูู. ุณูุนุงุฏ ุงููุญุงููุฉ.")
            if attempt < MAX_RETRIES - 1:
                time.sleep(5)
            else:
                # [ุชุนุฏูู ุงูุฅุฑุฌุงุน] ูู ุญุงูุฉ ุงููุดู ูุฑุฌุน ุจูุงูุงุช ูุงุฑุบุฉ ูุชูููุงุช 0
                return {}, 0, 0 
                
        except Exception as e:
            raise Exception(f"ุญุฏุซ ุฎุทุฃ ุนุงู ุฃุซูุงุก ุงุณุชุฏุนุงุก API: {e}")
    
    # [ุฅุฑุฌุงุน ุงููุดู] ุฅุฐุง ูู ุชูุฌุญ ุฃู ูุญุงููุฉ
    return {}, 0, 0 


# ... (ุจุงูู ุงูุฏูุงู load_yaml_and_content ู update_alu_file ุชุจูู ููุง ูู) ...

def load_yaml_and_content(file_path):
    """ุชุญููู ุฑุฃุณ YAML ููุญุชูู ุงููุต ูู ููู Markdown"""
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    match = re.search(r'^---\n(.*?)\n---\n(.*)', content, re.DOTALL)
    
    if match:
        yaml_header = match.group(1)
        text_content = match.group(2)
        try:
            metadata = yaml.safe_load(yaml_header)
            return metadata, text_content
        except yaml.YAMLError as e:
            print(f"ุฎุทุฃ ูู ุชุญููู YAML ููููู {file_path}: {e}")
            return None, content
    
    return None, content

def update_alu_file(file_path, new_metadata, text_content):
    """ุชุญุฏูุซ ููู ALU ุจุงูุจูุงูุงุช ุงููุตููุฉ ุงูุฌุฏูุฏุฉ"""
    
    # 1. ุชุญุฏูุซ ุญููู ุงูููุฎุต ูุงูุชุตุญูุญุงุช
    new_metadata['summary'] = new_metadata.get('summary', 'ุชู ุชุญุฏูุซ ุงูููุฎุต ุจูุงุณุทุฉ LLM.')
    new_metadata['keywords'] = new_metadata.get('keywords', [])
    new_metadata['aspect'] = new_metadata.get('aspect', 'ุบูุฑ ูุตูู')
    new_metadata['ocr_corrections'] = new_metadata.get('ocr_corrections', {})

    # 2. ุฅุนุงุฏุฉ ุฅูุดุงุก ุฑุฃุณ YAML
    updated_yaml_header = create_yaml_header(new_metadata)
    
    # 3. ุฏูุฌ YAML ูุงููุญุชูู ุงููุตู
    final_content = updated_yaml_header + text_content.strip()
    
    with open(file_path, 'w', encoding='utf-8') as f:
        f.write(final_content)


# *******************************************************************
# ุงููุธููุฉ ุงูุฑุฆูุณูุฉ ุงูููุญุฏููุซุฉ (ูุน ุชุฌููุน ุงูุชูููุงุช)
# *******************************************************************

def process_enrichment(input_folder="processed_systems_output"):
    """ุงููุธููุฉ ุงูุฑุฆูุณูุฉ ูุชุดุบูู ุงูุฅุซุฑุงุก ุนูู ุฌููุน ุงููุซุงุฆู ุฏุงุฎู ุงููุฌูุฏุงุช ุงููุฑุนูุฉ."""
    
    base_path = Path(input_folder)
    source_path = Path("source_files")
    
    # [ุฅุถุงูุฉ ุฌุฏูุฏุฉ] ูุชุบูุฑุงุช ุชุฌููุน ุงูุชูููุงุช
    total_input_tokens_grand = 0 
    total_output_tokens_grand = 0 

    if not base_path.exists():
        print(f"โ ูู ูุชู ุงูุนุซูุฑ ุนูู ูุฌูุฏ ุงููุฎุฑุฌุงุช: {input_folder}")
        return

    doc_folders = [d for d in base_path.iterdir() if d.is_dir() and 'ูุซููุฉ-' in d.name]

    if not doc_folders:
        print(f"โ ูู ูุชู ุงูุนุซูุฑ ุนูู ุฃู ูุฌูุฏุงุช ูุซุงุฆู (ุชุจุฏุฃ ุจู 'ูุซููุฉ-') ูู ูุฌูุฏ {input_folder}.")
        return

    print(f"โ ุชู ุชุฌููุน {len(doc_folders)} ูุซููุฉ ุฌุงูุฒุฉ ููุฅุซุฑุงุก.")
    
    total_processed = 0
    
    # 2. ุงููุฑุญูุฉ ุงูุซุงููุฉ: ูุนุงูุฌุฉ ูู ูุซููุฉ ุนูู ุญุฏุฉ
    for doc_folder in doc_folders:
        doc_slug = doc_folder.name
        
        # [ุฅุถุงูุฉ ุฌุฏูุฏุฉ] ูุชุบูุฑุงุช ุชุฌููุน ุงูุชูููุงุช ุนูู ูุณุชูู ุงููุซููุฉ
        doc_input_tokens = 0
        doc_output_tokens = 0
        
        print(f"\n" + "="*70)
        print(f"--- ุจุฏุก ุงูุฅุซุฑุงุก ูุงูุฑูุงุจุท ูููุซููุฉ: {doc_slug} ---")
        
        # ุชุญููู ุงูุณูุงู ุงูุฃุณุงุณู ูุฑุฉ ูุงุญุฏุฉ ููู ูุซููุฉ
        core_context = get_core_context(doc_slug, source_folder=source_path)
        
        # ุฃ. ุฅูุฌุงุฏ ูุชุฑุชูุจ ุฌููุน ูููุงุช ALU ุฏุงุฎู ูุฐุง ุงููุฌูุฏ ุงููุฑุนู
        alu_list = []
        alu_files = sorted(doc_folder.glob(f"{doc_slug}*--ูุงุฏุฉ-*.md")) 

        for file_path in alu_files:
            metadata, _ = load_yaml_and_content(file_path)
            if not metadata: continue

            article_range_match = re.search(r'--ูุงุฏุฉ-(\d+)', metadata.get('id', ''))
            sort_key = int(article_range_match.group(1)) if article_range_match else 0
            
            alu_list.append({'id': metadata.get('id'), 'path': file_path, 'sort_key': sort_key})

        alu_list.sort(key=lambda x: x['sort_key'])

        if not alu_list:
            print(f" ยโ ูู ูุชู ุงูุนุซูุฑ ุนูู ุฃู ูููุงุช ALU (ูุงุฏุฉ) ุตุงูุญุฉ ูููุซููุฉ {doc_slug}. ุชุฎุทู.")
            continue

        print(f" ย> ุชู ุงูุนุซูุฑ ุนูู {len(alu_list)} ูุงุฏุฉ ุฌุงูุฒุฉ ูููุนุงูุฌุฉ.")
        
        # ุจ. ูุนุงูุฌุฉ ุงูุฅุซุฑุงุก (ุงูุฑูุงุจุท ู LLM)
        all_doc_ocr_corrections = [] 
        
        for i, alu_data in enumerate(alu_list):
            current_path = alu_data['path']
            
            prev_id = alu_list[i-1]['id'] if i > 0 else None
            next_id = alu_list[i+1]['id'] if i < len(alu_list) - 1 else None
            
            metadata, text_content = load_yaml_and_content(current_path)
            
            if metadata:
                # ุชุญุฏูุซ ุงูุฑูุงุจุท
                metadata['prev'] = prev_id
                metadata['next'] = next_id
                
                article_text_for_llm = text_content.strip()
                
                try:
                    # [ุชุนุฏูู] ุงุณุชูุจุงู ุจูุงูุงุช LLM ูุงูุชูููุงุช
                    llm_data, input_tokens, output_tokens = call_gemini_api(article_text_for_llm, core_context)
                    
                    # [ุฅุถุงูุฉ ุฌุฏูุฏุฉ] ุชุฌููุน ุงูุชูููุงุช ูููุญุงููุฉ ุงููุงุฌุญุฉ
                    doc_input_tokens += input_tokens
                    doc_output_tokens += output_tokens
                    
                    # ุฏูุฌ ุจูุงูุงุช LLM ูู ุงูููุชุงุฏุงุชุง
                    metadata['summary'] = llm_data.get('summary', metadata.get('summary'))
                    metadata['keywords'] = llm_data.get('keywords', metadata.get('keywords', []))
                    metadata['aspect'] = llm_data.get('aspect', metadata.get('aspect', 'ุบูุฑ ูุตูู'))
                    
                    llm_corrections = llm_data.get('ocr_corrections', [])
                    
                    # 1. ุชุญุฏูุซ ocr_corrections ูู ุฑุฃุณ YAML
                    metadata['ocr_corrections'] = {c['original_word']: c['suggested_correction'] for c in llm_corrections if 'original_word' in c and 'suggested_correction' in c}
                    
                    # 2. ุชุฌููุน ุงูุจูุงูุงุช ูููู ocr_review.json
                    article_number = metadata.get('articles', metadata.get('id').split('--ูุงุฏุฉ-')[-1])
                    for correction in llm_corrections:
                        correction_record = correction.copy()
                        correction_record['file'] = current_path.name
                        correction_record['article_number'] = article_number
                        all_doc_ocr_corrections.append(correction_record)


                    # ุชุญุฏูุซ ุงูููู ุจุงููุงูู
                    update_alu_file(current_path, metadata, text_content)
                    print(f" ยโ ุชู ุชุญุฏูุซ ูุฅุซุฑุงุก ุงูููู: {current_path.name}")
                    total_processed += 1
                
                except Exception as e:
                    # ุฅุฐุง ูุดู LLM ุจุนุฏ ูู ุงููุญุงููุงุช (ุชู ุงูุฅุนูุงู ุนู ุฐูู ูู ุฏุงูุฉ call_gemini_api)
                    print(f" ยโ ูุดู ุฅุซุฑุงุก ุงูููู {current_path.name} ุจุนุฏ ุงููุญุงููุงุช. ุงูุฎุทุฃ: {e}")
                    # ุงุณุชูุฑุงุฑ ุงูุชุญุฏูุซ ุจุงูุฑูุงุจุท ุญุชู ูู ูุดู LLM
                    update_alu_file(current_path, metadata, text_content)
                    total_processed += 1
        
        # ุชุฌููุน ุชูููุงุช ุงููุซููุฉ ูู ุงููุฌููุน ุงูููู
        total_input_tokens_grand += doc_input_tokens
        total_output_tokens_grand += doc_output_tokens

        # ุฌ. ุญูุธ ููู ocr_review.json ุจุนุฏ ูุนุงูุฌุฉ ุฌููุน ุงูููุงุฏ
        save_ocr_review_file(doc_slug, all_doc_ocr_corrections, doc_folder)
        
        # [ุฅุถุงูุฉ ุฌุฏูุฏุฉ] ุทุจุงุนุฉ ููุฎุต ุชูููุงุช ุงููุซููุฉ
        print("\n" + "๐ธ ููุฎุต ุงุณุชููุงู ุงููุซููุฉ ุงูุญุงููุฉ:")
        print(f"ุชูููุงุช ุงููุฏุฎู (Input Tokens): {doc_input_tokens}")
        print(f"ุชูููุงุช ุงููุฎุฑุฌ (Output Tokens): {doc_output_tokens}")
        print("--------------------------------------------------")


    print("\n" + "="*70)
    print(f"โ ุงูุชูู ุงูุฅุซุฑุงุก ุงูุฏูุนู. ุชู ุชุญุฏูุซ {total_processed} ููู ALU ูู {len(doc_folders)} ูุซููุฉ.")
    
    # [ุฅุถุงูุฉ ุฌุฏูุฏุฉ] ุทุจุงุนุฉ ููุฎุต ุงูุชูููุฉ ุงูููุงุฆู (ูููุจุฑูุฌ)
    print("\n" + "๐ฐ ููุฎุต ุงูุชูููุฉ ุงูุฅุฌูุงูู (Token Usage):" + "\n" + "="*70)
    print(f"ุชูููุงุช ุงููุฏุฎู ุงูููู (Input Tokens): {total_input_tokens_grand}")
    print(f"ุชูููุงุช ุงููุฎุฑุฌ ุงูููู (Output Tokens): {total_output_tokens_grand}")
    print(f"ุฅุฌูุงูู ุงูุชูููุงุช ุงููุณุชุฎุฏูุฉ: {total_input_tokens_grand + total_output_tokens_grand}")
    print("==========================================================")

# --- ุงูุชุดุบูู ุงูููุญุณููู ---
if __name__ == "__main__":
    print("โ ุชู ุชุญููู ุงูููุฏ ุจูุฌุงุญ. ุจุฏุก ุงููุนุงูุฌุฉ ุงูุฏูุนูุฉ...")
    
    try:
        process_enrichment()
    except Exception as e:
        print("\n" + "="*70)
        print("--- ุฎุทุฃ ูุงุฏุญ ุบูุฑ ูุชููุน ุฃุซูุงุก ุชุดุบูู ุงููุนุงูุฌ ---")
        print(f"โ ุชุนุซุฑ ุงูุณูุฑุจุช ุนูุฏ ูุฐู ุงูููุทุฉ: {e}")
        print("="*70)
        traceback.print_exc()